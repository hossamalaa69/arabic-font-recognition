{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.signal import convolve2d\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBorderColor(img):\n",
    "    border = np.asarray(img[0,:])\n",
    "    border = np.concatenate((border, np.asarray(img[-1, :])))\n",
    "    border = np.concatenate((border, np.asarray(img[:, 0])))\n",
    "    border = np.concatenate((border, np.asarray(img[:, -1])))\n",
    "    return np.bincount(border).argmax()\n",
    "\n",
    "\n",
    "def pre_processing(img):\n",
    "    _, img_binarized = cv2.threshold(img, 120, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)    \n",
    "    if(getBorderColor(img_binarized) != 0):\n",
    "        img_binarized = cv2.bitwise_not(img_binarized) \n",
    "    \n",
    "    kernel = np.ones((30,30), np.uint8)\n",
    "    img_dilation = cv2.dilate(img_binarized, kernel, iterations=1)\n",
    "    cnts = cv2.findContours(img_dilation, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[0]\n",
    "    cntsSorted = sorted(cnts, key=lambda x: cv2.contourArea(x), reverse=True)\n",
    "    (x,y,w,h) = cv2.boundingRect(cntsSorted[0])\n",
    "    return img_binarized[y:y+h, x:x+w]    \n",
    "\n",
    "def showImage(img):\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = []\n",
    "Y = []\n",
    "for i in range(1, 10):\n",
    "    for filename in os.listdir(\"ACdata_base/\" + str(i)):\n",
    "        img = cv2.imread(os.path.join(\"ACdata_base/\" + str(i),filename), cv2.IMREAD_GRAYSCALE)\n",
    "        if img is not None:\n",
    "            data_set.append(pre_processing(img))\n",
    "            Y.append(i-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_testValid, Y_train, Y_testValid = train_test_split(data_set, Y, test_size=0.4, random_state=13)\n",
    "X_validation, X_test, Y_validation, Y_test = train_test_split(X_testValid, Y_testValid, test_size=0.5, random_state=38)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique, counts = np.unique(np.array(Y_train), return_counts=True)\n",
    "# print(counts)\n",
    "# unique, counts1 = np.unique(np.array(Y_validation), return_counts=True)\n",
    "# print(counts1)\n",
    "# unique, counts2 = np.unique(np.array(Y_test), return_counts=True)\n",
    "# print(counts2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lpq(img, winSize=3, freqestim=1, mode='nh'):\n",
    "    \n",
    "    rho=0.90\n",
    "\n",
    "    STFTalpha=1/winSize  # alpha in STFT approaches (for Gaussian derivative alpha=1)\n",
    "    sigmaS=(winSize-1)/4 # Sigma for STFT Gaussian window (applied if freqestim==2)\n",
    "    sigmaA=8/(winSize-1) # Sigma for Gaussian derivative quadrature filters (applied if freqestim==3)\n",
    "\n",
    "    convmode='valid' # Compute descriptor responses only on part that have full neigborhood. Use 'same' if all pixels are included (extrapolates np.image with zeros).\n",
    "\n",
    "    img=np.float64(img) # Convert np.image to double\n",
    "    r=(winSize-1)/2 # Get radius from window size\n",
    "    x=np.arange(-r,r+1)[np.newaxis] # Form spatial coordinates in window\n",
    "\n",
    "    if freqestim==1:  #  STFT uniform window\n",
    "        #  Basic STFT filters\n",
    "        w0=np.ones_like(x)\n",
    "        w1=np.exp(-2*np.pi*x*STFTalpha*1j)\n",
    "        w2=np.conj(w1)\n",
    "\n",
    "    ## Run filters to compute the frequency response in the four points. Store np.real and np.imaginary parts separately\n",
    "    # Run first filter\n",
    "    filterResp1=convolve2d(convolve2d(img,w0.T,convmode),w1,convmode)\n",
    "    filterResp2=convolve2d(convolve2d(img,w1.T,convmode),w0,convmode)\n",
    "    filterResp3=convolve2d(convolve2d(img,w1.T,convmode),w1,convmode)\n",
    "    filterResp4=convolve2d(convolve2d(img,w1.T,convmode),w2,convmode)\n",
    "\n",
    "    # Initilize frequency domain matrix for four frequency coordinates (np.real and np.imaginary parts for each frequency).\n",
    "    freqResp=np.dstack([filterResp1.real, filterResp1.imag,\n",
    "                        filterResp2.real, filterResp2.imag,\n",
    "                        filterResp3.real, filterResp3.imag,\n",
    "                        filterResp4.real, filterResp4.imag])\n",
    "\n",
    "    ## Perform quantization and compute LPQ codewords\n",
    "    inds = np.arange(freqResp.shape[2])[np.newaxis,np.newaxis,:]\n",
    "    LPQdesc=((freqResp>0)*(2**inds)).sum(2)\n",
    "\n",
    "    ## Switch format to uint8 if LPQ code np.image is required as output\n",
    "    if mode=='im':\n",
    "        LPQdesc=np.uint8(LPQdesc)\n",
    "\n",
    "    ## Histogram if needed\n",
    "    if mode=='nh' or mode=='h':\n",
    "        LPQdesc=np.histogram(LPQdesc.flatten(),range(256))[0]\n",
    "\n",
    "    ## Normalize histogram if needed\n",
    "    if mode=='nh':\n",
    "        LPQdesc=LPQdesc/LPQdesc.sum()\n",
    "    \n",
    "    return LPQdesc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeatures(img):\n",
    "    return lpq(img)\n",
    "\n",
    "def getFeaturesList(images):\n",
    "   features = []\n",
    "   for i in range(len(images)):\n",
    "        features.append(getFeatures(images[i]))\n",
    "   return np.asarray(features)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1011, 255)\n",
      "(337, 255)\n",
      "(337, 255)\n"
     ]
    }
   ],
   "source": [
    "train_features = getFeaturesList(X_train)\n",
    "validation_features = getFeaturesList(X_validation)\n",
    "test_features = getFeaturesList(X_test)\n",
    "\n",
    "print(train_features.shape)\n",
    "print(validation_features.shape)\n",
    "print(test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hossa\\AppData\\Local\\Temp/ipykernel_8320/1770282767.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_validation = torch.tensor(Y_validation)\n"
     ]
    }
   ],
   "source": [
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H1, D_out = train_features.shape[0], train_features.shape[1], 128, 9\n",
    "\n",
    "x = torch.tensor(train_features).double()\n",
    "y = torch.tensor(Y_train)\n",
    "\n",
    "x_validation = torch.tensor(validation_features).double()\n",
    "Y_validation = torch.tensor(Y_validation)\n",
    "\n",
    "x_test = torch.tensor(test_features).double()\n",
    "y_test = torch.tensor(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H1),\n",
    "    torch.nn.ReLU(),    \n",
    "    torch.nn.Linear(H1, D_out),\n",
    "    torch.nn.LogSoftmax(dim=1)\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.NLLLoss()\n",
    "\n",
    "learning_rate = 2*1e-5\n",
    "\n",
    "model.to(device)\n",
    "model.double()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "training_losses = []\n",
    "validation_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 0.030153019979845614   [Training]\n",
      "500 0.13599264626317686   [Validation]\n",
      "1000 0.029773852855023847   [Training]\n",
      "1000 0.13588646556898748   [Validation]\n",
      "1500 0.029400212699370733   [Training]\n",
      "1500 0.13578644614170082   [Validation]\n",
      "2000 0.029031459059672117   [Training]\n",
      "2000 0.13570067741713507   [Validation]\n",
      "2500 0.02866763631965883   [Training]\n",
      "2500 0.13561614043227985   [Validation]\n",
      "3000 0.02830970228689304   [Training]\n",
      "3000 0.13551935779086072   [Validation]\n",
      "3500 0.027957030681080595   [Training]\n",
      "3500 0.13542713450007124   [Validation]\n",
      "4000 0.02760952068291685   [Training]\n",
      "4000 0.1353375932768716   [Validation]\n",
      "4500 0.027266268602053934   [Training]\n",
      "4500 0.13525685532864976   [Validation]\n",
      "5000 0.026928143616453105   [Training]\n",
      "5000 0.13517575940264154   [Validation]\n",
      "5500 0.026595329821991892   [Training]\n",
      "5500 0.13509811268460922   [Validation]\n",
      "6000 0.026267464456733173   [Training]\n",
      "6000 0.13502182387824635   [Validation]\n",
      "6500 0.025944388147620394   [Training]\n",
      "6500 0.13495021803260157   [Validation]\n",
      "7000 0.025626037716192557   [Training]\n",
      "7000 0.1348862504685506   [Validation]\n",
      "7500 0.02531241967008378   [Training]\n",
      "7500 0.13482114905057446   [Validation]\n",
      "8000 0.02500349738894002   [Training]\n",
      "8000 0.1347483143346673   [Validation]\n",
      "8500 0.024698427817602182   [Training]\n",
      "8500 0.1346900079997118   [Validation]\n",
      "9000 0.02439877841518894   [Training]\n",
      "9000 0.1346257799940017   [Validation]\n",
      "9500 0.024103755044165117   [Training]\n",
      "9500 0.13456560782761823   [Validation]\n",
      "10000 0.023813466934627218   [Training]\n",
      "10000 0.13450510061151819   [Validation]\n",
      "10500 0.023527765755107493   [Training]\n",
      "10500 0.13445183936790786   [Validation]\n",
      "11000 0.023246439435925394   [Training]\n",
      "11000 0.13440289204175562   [Validation]\n",
      "11500 0.022969389678956568   [Training]\n",
      "11500 0.13436026954967242   [Validation]\n",
      "12000 0.022696537000154813   [Training]\n",
      "12000 0.1343219147991623   [Validation]\n",
      "12500 0.02242790361574299   [Training]\n",
      "12500 0.13429045510944648   [Validation]\n",
      "13000 0.022163331884914508   [Training]\n",
      "13000 0.13426868849607423   [Validation]\n",
      "13500 0.021902761686852863   [Training]\n",
      "13500 0.13425499649403833   [Validation]\n",
      "14000 0.021646141254508946   [Training]\n",
      "14000 0.13424787719197845   [Validation]\n",
      "14500 0.02139341960469471   [Training]\n",
      "14500 0.1342478717384967   [Validation]\n",
      "15000 0.02114444761341173   [Training]\n",
      "15000 0.13425741905316238   [Validation]\n"
     ]
    }
   ],
   "source": [
    "for layer in model.children():\n",
    "   if hasattr(layer, 'reset_parameters'):\n",
    "       layer.reset_parameters()\n",
    "\n",
    "for t in range(100000):\n",
    "    #print(t)\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    \n",
    "    y_pred = model(x)\n",
    "\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 500 == 499:\n",
    "        print(t+1, loss.item(),\"  [Training]\")\n",
    "        training_losses.append(loss.item())\n",
    " \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if t % 500 == 499:\n",
    "        with torch.no_grad():\n",
    "            x_valid = x_validation.to(device)\n",
    "            y_valid = Y_validation.to(device)\n",
    "            y_pred2 = model(x_valid)\n",
    "            loss = loss_fn(y_pred2, y_valid)\n",
    "            print(t+1, loss.item(),\"  [Validation]\" )\n",
    "            validation_losses.append(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_losses, '-r')\n",
    "plt.plot(validation_losses, '-g')\n",
    "plt.show()         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy =  99.90108803165182 %\n",
      "Validation Accuracy =  97.03264094955489 %\n",
      "Test Accuracy =  98.51632047477746 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_pred = model(x)\n",
    "    y_pred = torch.argmax(torch.exp(y_pred), dim=1)\n",
    "    acc = (torch.sum(y_pred==y).item() / len(y)) * 100\n",
    "    print(\"Training Accuracy = \", acc, \"%\")    \n",
    "\n",
    "    x_valid = x_validation.to(device)\n",
    "    y_valid = Y_validation.to(device)\n",
    "    y_pred = model(x_valid)\n",
    "    y_pred = torch.argmax(torch.exp(y_pred), dim=1)\n",
    "    acc = (torch.sum(y_pred==y_valid).item() / len(y_valid)) * 100\n",
    "    print(\"Validation Accuracy = \", acc, \"%\")\n",
    "\n",
    "    x_test = x_test.to(device)\n",
    "    y_test = y_test.to(device)\n",
    "\n",
    "    y_pred = model(x_test)\n",
    "    y_pred = torch.argmax(torch.exp(y_pred), dim=1)\n",
    "    acc = (torch.sum(y_pred==y_test).item() / len(y_test)) * 100\n",
    "    print(\"Test Accuracy = \", acc, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = torch.load(\"model.pt\")\n",
    "# for name, param in model2.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name, param.data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy =  99.90108803165182 %\n",
      "Validation Accuracy =  97.03264094955489 %\n",
      "Test Accuracy =  98.51632047477746 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_pred = model2(x)\n",
    "    y_pred = torch.argmax(torch.exp(y_pred), dim=1)\n",
    "    acc = (torch.sum(y_pred==y).item() / len(y)) * 100\n",
    "    print(\"Training Accuracy = \", acc, \"%\")    \n",
    "\n",
    "    x_valid = x_validation.to(device)\n",
    "    y_valid = Y_validation.to(device)\n",
    "    y_pred = model2(x_valid)\n",
    "    y_pred = torch.argmax(torch.exp(y_pred), dim=1)\n",
    "    acc = (torch.sum(y_pred==y_valid).item() / len(y_valid)) * 100\n",
    "    print(\"Validation Accuracy = \", acc, \"%\")\n",
    "\n",
    "    x_test = x_test.to(device)\n",
    "    y_test = y_test.to(device)\n",
    "\n",
    "    y_pred = model2(x_test)\n",
    "    y_pred = torch.argmax(torch.exp(y_pred), dim=1)\n",
    "    acc = (torch.sum(y_pred==y_test).item() / len(y_test)) * 100\n",
    "    print(\"Test Accuracy = \", acc, \"%\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "af19850de543c87aa9c5433cdfdc668182c4fce690f4a520383c35fe24f2761b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
